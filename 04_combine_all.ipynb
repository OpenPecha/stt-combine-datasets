{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the data into once dataset\n",
    "\n",
    "Once you've run the following notebooks,\n",
    "- 01_stt_pecha_tools.ipynb\n",
    "- 02_prodigy.ipynb\n",
    "- 03_mv_saymore.ipynb\n",
    "\n",
    "You get the following tsv files\n",
    "- 01_stt_pecha_tools.tsv\n",
    "- 02_prodigy.tsv\n",
    "- 03_mv_saymore.tsv\n",
    "\n",
    "Concatenate the tsv files and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_pecha_tools = \"01_stt_pecha_tools.tsv\"\n",
    "prodigy =         \"02_prodigy.tsv\"\n",
    "mv =              \"03_mv_saymore.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pecha_tools_df = pd.read_csv(stt_pecha_tools, sep='\\t')\n",
    "prodigy_df = pd.read_csv(prodigy, sep='\\t')\n",
    "mv_df = pd.read_csv(mv, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pecha_tools_df, prodigy_df, mv_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_apples = pd.read_csv('04_bad_apples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['file_name'].isin(bad_apples['file_name'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates, duplicates were introduced from prodigy annotation tool. One of the reason we had to move away from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='file_name', keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the combined tsv\n",
    "\n",
    "Remove unwanted characters and remove transcriptions with english characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_transcription(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.strip()\n",
    "\n",
    "    text = re.sub(r\"་+\", \"་\", text)\n",
    "    text = re.sub(r\"།+\", \"།\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+།\", \"།\", text)\n",
    "\n",
    "    text = re.sub(r\"ཧཧཧ+\", \"ཧཧཧ\", text)\n",
    "    text = re.sub(r'འེ་འེ་(འེ་)+', r'འེ་འེ་འེ་', text)\n",
    "    text = re.sub(r'ཧ་ཧ་(ཧ་)+', r'ཧ་ཧ་ཧ་', text)\n",
    "\n",
    "    chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\/\\{\\}\\(\\)\\༽\\》\\༼\\《\\༅\\༄\\༈\\༑\\༠]'\n",
    "    \n",
    "    text = re.sub(chars_to_ignore_regex, '', text)+\" \"\n",
    "    return text\n",
    "\n",
    "def check_if_regex(text):\n",
    "    text = str(text)\n",
    "    regex = re.compile(r'[a-zA-Z]+')\n",
    "    match = re.search(regex, text)\n",
    "    \n",
    "    return bool(match)\n",
    "    \n",
    "print(check_if_regex('ཧཧ'))\n",
    "print(check_if_regex('some text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['uni'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['uni'].apply(check_if_regex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['uni'] = df['uni'].map(clean_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyewts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert unicode tibetan characters into wylie format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyewts\n",
    "\n",
    "converter = pyewts.pyewts()\n",
    "\n",
    "df['wylie'] = df['uni'].apply(converter.toWylie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_len'] = df['uni'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_len'].hist(bins=100, range=(0, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cutoff = 400\n",
    "lower_cutoff = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['char_len'] > upper_cutoff].shape[0], df[df['char_len'] < lower_cutoff].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['char_len'] > upper_cutoff][['uni', 'url', 'char_len']].sort_values(by='char_len').to_csv('04_longer_than_upper_cutoff.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['char_len'] < lower_cutoff][['uni', 'url', 'char_len']].sort_values(by='char_len').to_csv('04_shorter_than_lower_cutoff.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['char_len'] < upper_cutoff) & (df['char_len'] > lower_cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the audio time duration from the file name. \n",
    "\n",
    "There is to format for encoding the time span. We use the one with \\_to\\_ now. The difference came from the using a different library to do the Voice Activity Detection and splitting the audio for Tibetan Teachings. We have since started using pyannote-audio for all departments now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSpan(filename):\n",
    "\n",
    "    filename = filename.replace(\".wav\", \"\")\n",
    "    filename = filename.replace(\".WAV\", \"\")\n",
    "    filename = filename.replace(\".mp3\", \"\")\n",
    "    filename = filename.replace(\".MP3\", \"\")\n",
    "    try:\n",
    "        if \"_to_\" in filename:\n",
    "            start, end = filename.split(\"_to_\")\n",
    "            start = start.split(\"_\")[-1]\n",
    "            end = end.split(\"_\")[0]\n",
    "            end = float(end)\n",
    "            start = float(start)\n",
    "            return abs(end - start)/1000\n",
    "        else:\n",
    "            start, end = filename.split(\"-\")\n",
    "            start = start.split(\"_\")[-1]\n",
    "            end = end.split(\"_\")[0]\n",
    "            end =   float(end)\n",
    "            start = float(start)\n",
    "            return abs(end - start)\n",
    "    except Exception as err:\n",
    "        print(f\"filename is:'{filename}'. Could not parse to get time span.\")\n",
    "        return 0\n",
    "    \n",
    "\n",
    "getTimeSpan(\"STT_TT00031_03471.850-03477.44\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_len'] = df['file_name'].apply(getTimeSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_len'].hist(bins=100, range=(0, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = df['url'].map(lambda x : x.replace('#','%23'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install botok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [botok](https://github.com/OpenPecha/Botok) to get count of non tibetan syllables and illegal tibetan syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botok import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    non_word_count = sum(1 for token in tokens if token.pos == 'NON_WORD' and not token.skrt)\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    non_bo_word_count = 0\n",
    "    for token in tokens:\n",
    "        if token.chunk_type in [\"LATIN\", \"CJK\", \"OTHER\"] and (\n",
    "            token.chunk_type != \"OTHER\" or not token.skrt\n",
    "        ):\n",
    "            non_bo_word_count += 1\n",
    "    \n",
    "    return non_word_count, non_bo_word_count, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer()\n",
    "\n",
    "df['non_word_count'], df['non_bo_word_count'], df['total_tokens'] = zip(*df['uni'].apply(lambda text: process_transcript(text, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('04_combine_all.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['non_word_count'] > 1].shape[0], df[ df['non_bo_word_count'] > 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['non_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['non_bo_word_count'] > 1 ].loc[:, ('file_name', 'uni')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['non_word_percentage'] = (df['non_word_count'] / df['total_tokens']) * 100\n",
    "# df.fillna(0, inplace=True)  # Replace NaN values with 0 in case of division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('04_combine_all.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['non_word_count'] > 1].shape[0] / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word = df[df['non_word_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word.groupby('dept').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word.groupby('dept').sum('audio_len')['audio_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word.groupby('dept').size() / df.groupby('dept').size() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dept').size() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word = df_non_word.sort_values(by='non_word_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_word.to_csv('04_non_word_count.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train), len(val), len(test), len(train)+len(val)+len(test), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('train.tsv', sep='\\t', index=False)\n",
    "# val.to_csv(  'val.tsv', sep='\\t', index=False)\n",
    "# test.to_csv( 'test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['uni'].str.contains('ཧཧཧ').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('04_combine_all.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dept')['audio_len'].sum()/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_len'].sum()/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by='audio_len', ascending=False, inplace=True)\n",
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:100,[0, 1, 3, 5]].to_csv(\"04_random_100.tsv\", index=False, sep='\\t')\n",
    "df.iloc[0:100,[0, 1, 3, 5]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['grade'] == 3].groupby('dept').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('04_combine_all.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dept').sum('audio_len')['audio_len']/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_len'].sum()/60/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
